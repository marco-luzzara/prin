services:

# ----------------------------- DASHBOARD FOR DATA LOADING -----------------------------
  user-dashboard:
    build:
      context: edge-vm
      dockerfile: ./Dockerfile
      target: base
    ports:
      - "${DATA_LOADING_WEBAPP_PORT}:80"
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    environment:
      - FLASK_SECRET_KEY=SecretKeyForFlaskSessions
      - KAFKA_BOOTSTRAP_SERVERS=kafka-broker-0:9092

# ----------------------------- KAFKA -----------------------------
# https://github.com/bitnami/containers/blob/466fffb6f2fe8cef54f281f22e62a967f7d69b78/bitnami/kafka/README.md

  kafka-broker-0:
    image: apache/kafka:${KAFKA_VERSION}
    working_dir: /opt/kafka/bin/
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server kafka-broker-0:9092" ]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 2m
    environment:
      KAFKA_NODE_ID: 0
      KAFKA_PROCESS_ROLES: 'broker,controller'
      # CLUSTER_ID: 'clusterid0with016bytes'
    volumes:
      - ./kafka/data/broker-0:/var/lib/kafka/data
      - ./kafka/configs:/mnt/shared/config

  kafka-init:
    image: apache/kafka:${KAFKA_VERSION}
    depends_on:
      kafka-broker-0:
        condition: service_healthy
    working_dir: /opt/kafka/bin
    volumes:
      - ./kafka/init.sh:/init.sh
    environment:
      - TOPIC__TASK_RESULT__RETENTION_SECONDS=${S3_PRE_SIGNED_URL_EXPIRATION_SECONDS}
    entrypoint: ["/init.sh"]

# ----------------------------- TASK DISPATCHER -----------------------------
  task-dispatcher:
    build:
      context: ./task-dispatcher
      args:
        KAFKA_VERSION: ${KAFKA_VERSION}
      dockerfile_inline: |
        ARG KAFKA_VERSION=
        FROM apache/kafka:${KAFKA_VERSION}
        # user root is necessary to use the docker socket
        USER root
        RUN apk add jq curl
        COPY . /dispatcher/
    entrypoint: ["/dispatcher/entrypoint.sh"]
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker-0:9092
      - TASK_DOCKER_IMAGE=prin-task

# ----------------------------- TEST TASK -----------------------------
  test-task:
    image: prin-task:latest
    depends_on:
      trino:
        condition: service_healthy
    environment:
      - TRINO_USER=trino
      - TRINO_ENDPOINT=trino:8080
      - TRINO_CATALOG=hive
      - TRINO_SCHEMA=default
      - TASK_APIS_BASE_URL=http://task-apis
    entrypoint: ["sleep", "inf"]

# ----------------------------- TASK APIS -----------------------------

  task-apis:
    build:
      context: task-apis
      dockerfile: ./Dockerfile
      target: base
    ports:
      - "${TASK_APIS_PORT}:80"
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-broker-0:9092 #,kafka-broker-1:9092
      - KAFKA_TOPIC=devprin.task.result
      - S3_ENDPOINT=http://minio:9000
      - S3_BUCKET=${MINIO_BUCKET}
      - S3_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - S3_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - S3_PRE_SIGNED_URL_EXPIRATION_SECONDS=${S3_PRE_SIGNED_URL_EXPIRATION_SECONDS}

# ----------------------------- HIVE METASTORE -----------------------------
  hive-metastore:
    build:
      args:
        HIVE_VERSION: ${HIVE_VERSION}
      context: ./hive/docker
    image: hive-with-postgres:${HIVE_VERSION}
    depends_on:
      ranger-db:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "cat /proc/net/tcp6 | sed -n '2p' | awk '{ print $$2 }' | grep 237B"]
      start_period: 1m
      retries: 5
      timeout: 5s
      interval: 5s
    environment:
      - IS_RESUME=${SKIP_HIVE_SCHEMA_INIT:-false}
      - SERVICE_NAME=metastore
      - HIVE_CUSTOM_CONF_DIR=/opt/hive/custom-conf
      - DB_DRIVER=postgres
      - DB_DRIVER_CLASSNAME=org.postgresql.Driver
      - DB_CONNECTION_URL=jdbc:postgresql://ranger-db:5432/hive_metastore_db
      - DB_USER=${HIVE_METASTORE_DATABASE_USER}
      - DB_PASSWORD=${HIVE_METASTORE_DATABASE_PASSWORD}
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - S3_ENDPOINT_URL=http://minio:9000
      - S3_BUCKET=${MINIO_BUCKET}
      - S3_PREFIX=metadata
    volumes:
      - ./hive/conf:/opt/hive/custom-conf
      - ./hive/hooks/atlas/import-hive.sh:/opt/hive/import-hive.sh
    # ports:
    #   - '9083:9083'


# ----------------------------- MINIO -----------------------------
  minio:
    image: quay.io/minio/minio:latest
    ports:
    #   - '9000:9000'
      - '${MINIO_PORT}:9001'
    # volumes:
    #   - './minio/data:/data'
    environment:
      - MINIO_ROOT_USER
      - MINIO_ROOT_PASSWORD
    healthcheck:
      test: curl --fail http://localhost:9000/minio/health/live
      start_period: 1m
      retries: 10
      timeout: 5s
      interval: 5s
    command: ["server", "/data", "--console-address", ":9001"]

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET=${MINIO_BUCKET}
    entrypoint: ["/init.sh"]
    volumes:
      - ./minio/init.sh:/init.sh


# ----------------------------- TRINO -----------------------------
# https://trino.io/docs/current/installation/containers.html

  trino:
    build:
      context: ./trino/docker/server
      args:
        TRINO_VERSION: ${TRINO_VERSION}
    stdin_open: true
    tty: true
    ports:
      - "${TRINO_PORT}:8080"
    healthcheck:
      test: ["CMD-SHELL", "/usr/lib/trino/bin/health-check"]
      interval: 10s
      start_period: 2m
      timeout: 5s
      retries: 10
    depends_on:
      ranger:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/kafka-description-files:/etc/trino/kafka
      - ./trino/configs/config.properties:/etc/trino/config.properties
      - ./trino/configs/log.properties:/etc/trino/log.properties
      - ./trino/configs/password-authenticator.properties:/etc/trino/password-authenticator.properties
      - ./trino/configs/password.db:/etc/trino/sec-conf/password.db
      - ./trino/https/server.pem:/etc/trino/sec-conf/server.pem
      - ./trino/plugins/ranger/access-control.properties:/etc/trino/access-control.properties
      - ./trino/plugins/ranger/ranger-policymgr-ssl.xml:/etc/trino/ranger-policymgr-ssl.xml
      - ./trino/plugins/ranger/ranger-trino-audit.xml:/etc/trino/ranger-trino-audit.xml
      - ./trino/plugins/ranger/ranger-trino-security.xml:/etc/trino/ranger-trino-security.xml

# ----------------------------- CDC -----------------------------
## Includes the Trino CLI. This container encompasses a CDC script
## that copies the new Kafka messages to a Trino table (pointing to an S3 bucket)

  cdc-script:
    build:
      context: ./trino/docker/cdc
      args:
        TRINO_VERSION: ${TRINO_VERSION}
    image: trino-cdc:${TRINO_VERSION}
    environment:
      - TRINO_ENDPOINT=http://trino:8080
      - FLUSH_DELAY_SECONDS=15
    depends_on:
      trino:
        condition: service_healthy
    volumes:
      - ./trino/docker/cdc/scripts:/cdc/scripts
      - ./trino/docker/cdc/data:/cdc/data

# ----------------------------- RANGER -----------------------------

  ranger:
    image: apache/ranger:${RANGER_VERSION}
    stdin_open: true
    tty: true
    ports:
      - "${RANGER_PORT}:6080"
    depends_on:
      ranger-zk:
        condition: service_started
      ranger-db:
        condition: service_healthy
      ranger-solr:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget --timeout 5 --spider --quiet http://localhost:6080 || exit 1"]
      interval: 10s
      start_period: 10m
      timeout: 5s
      retries: 10
    volumes:
      - ./ranger/install.properties:/opt/ranger/admin/install.properties
    environment:
      - DEBUG_ADMIN=${RANGER_DEBUG_ADMIN:-false}
      - RANGER_VERSION
      - RANGER_DB_TYPE=postgres
    command:
      - /home/ranger/scripts/ranger.sh

  ranger-zk:
    image: apache/ranger-zk:${RANGER_VERSION}
    # ports:
    #   - "2181:2181"

  ranger-solr:
    image: apache/ranger-solr:${RANGER_VERSION}
    # ports:
    #   - "8983:8983"
    volumes:
      - ./solr/data:/var/solr
    command:
      - solr-precreate
      - ranger_audits
      - /opt/solr/server/solr/configsets/ranger_audits/

  ranger-db:
    image: apache/ranger-db:${RANGER_VERSION}
    volumes:
      - ./postgres/data:/var/lib/postgresql/data
      - ./postgres/init/hive-metastore-init.sql:/docker-entrypoint-initdb.d/hive-metastore-init.sql
    healthcheck:
      test: 'su -c "pg_isready -q" postgres'
      interval: 10s
      timeout: 10s
      retries: 3

  ranger-tagsync:
    build:
      context: ranger/tagsync/docker
      target: ranger-tagsync
      args:
        TAGSYNC_VERSION: ${RANGER_VERSION}
    image: ranger-tagsync
    depends_on:
      ranger:
        condition: service_healthy
    volumes:
      - ./ranger/tagsync/configs/ranger-tagsync-install.properties:/opt/ranger/tagsync/install.properties
      - ./ranger/tagsync/configs/ranger-tagsync-tags.json:/opt/ranger/tagsync/data/tags.json
    environment:
      - TAGSYNC_VERSION=${RANGER_VERSION}
      - DEBUG_TAGSYNC=${DEBUG_TAGSYNC:-false}
      - TRINO_SERVICE_NAME=dev_trino
      - TRINO_CATALOG_NAME=hive
      - TRINO_SCHEMA_NAME=default

# ----------------------------- ATLAS  -----------------------------
## https://hub.docker.com/r/sburn/apache-atlas

  atlas:
    image: sburn/apache-atlas:2.3.0
    ports:
      - "${ATLAS_PORT}:21000"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:21000/login.jsp"]
      interval: 30s
      timeout: 10s
      start_period: 10m
      retries: 5
    # depends_on:
      # kafka-init:
      #   condition: service_completed_successfully
      # zookeeper:
      #   condition: service_healthy
    volumes:
      - ./atlas/conf/atlas-application.properties:/apache-atlas/conf/atlas-application.properties
      - ./atlas/conf/users-credentials.properties:/apache-atlas/conf/users-credentials.properties
      - ./atlas/data:/apache-atlas/data
      - ./atlas/logs:/apache-atlas/logs

  # zookeeper:
  #   image: zookeeper:3.9.3
  #   environment:
  #     - ZOO_4LW_COMMANDS_WHITELIST=*
  #   healthcheck:
  #     test: ["CMD", "echo", "ruok", "|", "nc", "-w", "2", "localhost", "2181"]
  #     interval: 20s
  #     timeout: 10s
  #     retries: 5

# ----------------------------- SUPERSET -----------------------------
## https://hub.docker.com/r/apache/superset

  superset:
    build:
      context: ./superset
      args:
        SUPERSET_VERSION: ${SUPERSET_VERSION}
    image: superset-with-trino:${SUPERSET_VERSION}
    ports:
      - "${SUPERSET_PORT}:8088"
    depends_on:
      trino:
        condition: service_healthy
    environment:
      - SUPERSET_SECRET_KEY=supersetkey
      - ADMIN_USERNAME=${SUPERSET_ADMIN_USERNAME}
      - ADMIN_PASSWORD=${SUPERSET_ADMIN_PASSWORD}
      - ADMIN_EMAIL=admin@superset.com
    volumes:
      - ./superset/bootstrap.sh:/bootstrap.sh
      # - ./superset/data:/app/superset_home/superset.db
    entrypoint: ["/bootstrap.sh"]
